general:
  enable_cuda: True  # disable this when running on machine without cuda
  torch_seed: 42
  cuda_seed: 42
  data_path: "data/kp20k/"

scheduling:
  batch_size: 64
  valid_batch_size: 64
  epoch: 10000
  model_checkpoint_path: 'saved_models/model.pt'

optimizer:
  step_rule: 'adam'  # adam, sgd
  learning_rate: 0.002
  learning_rate_decay_ratio: 0.8  # if valid loss goes up, decay the learning rate
  learning_rate_decay_from_this_epoch: 0
  learning_rate_decay_patience: 2  # be patient
  learning_rate_cut_lowerbound: 0.01  # if lr < lowerbound * init_lr, then stop cutting
  clip_grad_norm: 5

kp_generator:
  global:
    fast_rnns: True
    dropout_between_rnn_layers: 0.
    dropout_between_rnn_hiddens: 0.
    dropout_in_rnn_weights: 0.
    use_layernorm: False
    use_highway_connections: False

  embedding:
    word_level:
      path: '/home/eryua/fasttext-crawl-300d-2M.vec.h5' # no path because not using pretrained embeddings
      embedding_size: 300
      embedding_type: 'pretrained'  # pretrained, random
      embedding_trainable: True  # fix pretrained embedding
      embedding_dropout: 0.3
      embedding_oov_init: 'zero'  # zero, one, random, when certain tokens not in pretrained embedding vocabs

    char_level:
      embedding_size: 64
      embedding_rnn_size: [64]  # 32 on each direction
      embedding_trainable: True
      embedding_dropout: 0.3

  kp_generator:
    encoder_rnn_hidden_size: [256, 256]  # 128 on each direction, use an empty list to disable preproc_rnn, if you want..
    decoder_rnn_hidden_size: 384  # uni directional rnn
    decoder_init_states_generator_hidden_size: [256]
    decoder_vocab_generator_hidden_size: 384
    decoder_rnn_attention_size: 384
    pointer_softmax_hidden_size: 384
    history_info_integrator_hidden_size: [256]
